{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Final Project: Trumps Tweets\n",
    "\n",
    "**Group Members**: Sarah Chin, Maleah Fekete, Mason Watson, Jasper Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)\n",
    "\n",
    "import random\n",
    "random.seed(112358)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = pd.read_csv(\"data/trump-tweets/trump_tweet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>It was my great honor to deliver the keynote a...</td>\n",
       "      <td>10-25-2019 19:51:43</td>\n",
       "      <td>5132.0</td>\n",
       "      <td>16938</td>\n",
       "      <td>false</td>\n",
       "      <td>1.187819e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you @robertjeffress! https://t.co/o6mk8o...</td>\n",
       "      <td>10-25-2019 17:57:25</td>\n",
       "      <td>6095.0</td>\n",
       "      <td>22287</td>\n",
       "      <td>false</td>\n",
       "      <td>1.187790e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GaryCoby: Today @realDonaldTrump is receiv...</td>\n",
       "      <td>10-25-2019 17:35:13</td>\n",
       "      <td>5810.0</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>1.187785e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to South Carolina! https://t.co/CORtaP...</td>\n",
       "      <td>10-25-2019 17:30:21</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>40776</td>\n",
       "      <td>false</td>\n",
       "      <td>1.187783e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @gatewaypundit: Breaking Poll: 52% Say Impe...</td>\n",
       "      <td>10-25-2019 16:22:09</td>\n",
       "      <td>7649.0</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>1.187766e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text  \\\n",
       "0  Twitter for iPhone  It was my great honor to deliver the keynote a...   \n",
       "1  Twitter for iPhone  Thank you @robertjeffress! https://t.co/o6mk8o...   \n",
       "2  Twitter for iPhone  RT @GaryCoby: Today @realDonaldTrump is receiv...   \n",
       "3  Twitter for iPhone  Heading to South Carolina! https://t.co/CORtaP...   \n",
       "4  Twitter for iPhone  RT @gatewaypundit: Breaking Poll: 52% Say Impe...   \n",
       "\n",
       "            created_at  retweet_count favorite_count is_retweet        id_str  \n",
       "0  10-25-2019 19:51:43         5132.0          16938      false  1.187819e+18  \n",
       "1  10-25-2019 17:57:25         6095.0          22287      false  1.187790e+18  \n",
       "2  10-25-2019 17:35:13         5810.0              0       true  1.187785e+18  \n",
       "3  10-25-2019 17:30:21         8900.0          40776      false  1.187783e+18  \n",
       "4  10-25-2019 16:22:09         7649.0              0       true  1.187766e+18  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop this entry because it provides no information under the analyses we are currently performing\n",
    "trump_tweets = trump_tweets[~trump_tweets.text.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(trump_tweets[trump_tweets.retweet_count.isnull()])\n",
    "display(trump_tweets[trump_tweets.favorite_count.isnull()])\n",
    "display(trump_tweets[trump_tweets.created_at.isnull()])\n",
    "display(trump_tweets[trump_tweets.id_str.isnull()])\n",
    "\n",
    "# The entries with null retweet_count are the same as the entries will\n",
    "# null favorite_count and null created_at\n",
    "# These are also included in the entries with null id_str \n",
    "# We will group all of these into a separate dataframe to deal with null entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with null values\n",
    "trump_tweets_nulls = trump_tweets[trump_tweets.id_str.isnull()]\n",
    "\n",
    "# Drop indices with a lot of null entries from the main dataframe\n",
    "trump_tweets_main = trump_tweets[~trump_tweets.id_str.isnull()]\n",
    "\n",
    "# Check. These should all display an empty dataframe\n",
    "display(trump_tweets_main[trump_tweets_main.retweet_count.isnull()])\n",
    "display(trump_tweets_main[trump_tweets_main.favorite_count.isnull()])\n",
    "display(trump_tweets_main[trump_tweets_main.created_at.isnull()])\n",
    "display(trump_tweets_main[trump_tweets_main.id_str.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trump_tweets[trump_tweets.id_str.isnull() == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for this EDA we will ignore the 3 columns with all null non-text entries and the 3 columns where the categories got mixed up. We will only use data_main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets_main.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that favorite_count is an object instead of an number, so we need to convert it before continuing with EDA. We will also change created_at to a DateTime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets_main.is_retweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trump_tweets_main[trump_tweets_main['is_retweet'] == 'true'].shape)\n",
    "print(trump_tweets_main[trump_tweets_main['is_retweet'] == 'false'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets_main[(trump_tweets_main['is_retweet'] != 'true') & (trump_tweets_main['is_retweet'] != 'false')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first of these on the actual platform, it is not a retweet, but will need to do more probabilistic analysis to say with certainty. It does not make sense to just drop these rows as they were all created in a similar time frame so this could skew results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets_main[\"created_at\"] = pd.to_datetime(trump_tweets_main['created_at'], errors='coerce')\n",
    "trump_tweets_main[\"favorite_count\"] = pd.to_numeric(trump_tweets_main[\"favorite_count\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "trump_tweets_main[['favorite_count', 'retweet_count']] = scaler.fit_transform(trump_tweets_main[['favorite_count', 'retweet_count']])\n",
    "\n",
    "trump_tweets_main[\"fbyr\"] = np.log(trump_tweets_main[\"favorite_count\"] / trump_tweets_main[\"retweet_count\"])\n",
    "\n",
    "trump_tweets_main.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_retweets = trump_tweets_main[trump_tweets_main.is_retweet == 'true']\n",
    "trump_not_retweets = trump_tweets_main[trump_tweets_main.is_retweet == 'false']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_retweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_not_retweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create these for use in future analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trump_tweets_main['created_at'], trump_tweets_main[\"retweet_count\"])\n",
    "plt.xlabel(\"date of tweet\")\n",
    "plt.ylabel(\"retweet count\")\n",
    "plt.title(\"retweet count over time\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(trump_tweets_main['created_at'], trump_tweets_main[\"favorite_count\"])\n",
    "plt.xlabel(\"date of tweet\")\n",
    "plt.ylabel(\"favorite count\")\n",
    "plt.title(\"favorite count over time\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(trump_tweets_main['created_at'], trump_tweets_main[\"fbyr\"], )\n",
    "plt.ylabel(\"favorite/retweet ratio (log)\")\n",
    "plt.xlabel(\"date of tweet\")\n",
    "plt.title(\"favorite/retweet ratio (log) count over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Usable Features from Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        10-25-2019 19:51:43\n",
       "1        10-25-2019 17:57:25\n",
       "3        10-25-2019 17:30:21\n",
       "5        10-25-2019 12:54:24\n",
       "6        10-25-2019 12:32:06\n",
       "                ...         \n",
       "17814    08-21-2015 20:01:32\n",
       "17815    08-21-2015 16:35:21\n",
       "17816    08-21-2015 15:35:18\n",
       "17817    08-21-2015 15:25:01\n",
       "17818    08-21-2015 14:32:43\n",
       "Name: created_at, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets[trump_tweets['is_retweet'] == 'false'][\"created_at\"][0:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "other_excluders = [\"https\", \"thank\", \"co\"]\n",
    "\n",
    "trump_tweets = trump_tweets[trump_tweets['is_retweet'] == 'false'][0:15000]\n",
    "\n",
    "# Tokenize tweets and filter stop words using stop word list\n",
    "vectorizer = CountVectorizer(stop_words=stop_word_list + other_excluders)\n",
    "text_preprocessor = vectorizer.build_analyzer()\n",
    "tweet_vectors = np.zeros((15000, 300))\n",
    "tweet_text = trump_tweets['text'].dropna()\n",
    "\n",
    "for i in range(tweet_text.shape[0]):\n",
    "    tokenized_tweet = text_preprocessor(tweet_text.iloc[i])\n",
    "    n = len(tokenized_tweet)\n",
    "    word_vectors = []\n",
    "    \n",
    "    for j in range(n):\n",
    "        try:\n",
    "            word_vectors.append(w.get_vector(tokenized_tweet[j]))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(word_vectors) >= 1:\n",
    "        word_vectors = np.vstack(word_vectors)\n",
    "        tweet_vectors[i] = word_vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        It was my great honor to deliver the keynote a...\n",
       "1        Thank you @robertjeffress! https://t.co/o6mk8o...\n",
       "3        Heading to South Carolina! https://t.co/CORtaP...\n",
       "5        “Donald J. Trump is an absolutely historic Pre...\n",
       "6        Turkey fully understands not to fire on the Ku...\n",
       "                               ...                        \n",
       "17814    Leaving for Mobile Alabama right now - can't b...\n",
       "17815    Boston incident is terrible. We need energy an...\n",
       "17816    @AmyMek Every Time I see @realDonaldTrump addr...\n",
       "17817                       Great! https://t.co/oJ6sqHB3MA\n",
       "17818    We are going to have a wild time in Alabama to...\n",
       "Name: text, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-e18790f446dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mkmeanModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0minertias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeanModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 random_state=random_state)\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    443\u001b[0m     centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n\u001b[1;32m    444\u001b[0m                                             \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "from sklearn import metrics \n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "K = range(1,50, 5) \n",
    "inertias = []\n",
    "\n",
    "for k in K: \n",
    "    kmeanModel = KMeans(n_clusters=k).fit(tweet_vectors) \n",
    "    inertias.append(kmeanModel.inertia_) \n",
    "\n",
    "plt.plot(K, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanModel = KMeans(n_clusters=10).fit(tweet_vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ukraine', 0.7918287515640259),\n",
       " ('Moscow', 0.7575764656066895),\n",
       " ('Russian', 0.746496319770813),\n",
       " ('Belarus', 0.7303562760353088),\n",
       " ('Kremlin', 0.7048990726470947),\n",
       " ('Kazakhstan', 0.6979326009750366),\n",
       " ('Russians', 0.677611231803894),\n",
       " ('Biologist_Anatoly_Kochnev', 0.6745500564575195),\n",
       " ('Azerbaijan', 0.6726992726325989),\n",
       " ('Putin', 0.6636874675750732)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.most_similar(positive=['Russia'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet_text[20020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
